import aioredis
from googletrans import Translator
from your_module import StateGraph, get_openai_callback, ChatPromptTemplate, llm, contact, memory

# Establish a Redis connection
async def get_redis_connection(redis_url: str):
    return await aioredis.from_url(redis_url)

# Detect language function using googletrans (cached with Redis)
async def detect(text: str, redis) -> str:
    key = f"lang:{text}"
    cached_lang = await redis.get(key)
    
    if cached_lang:
        return cached_lang.decode("utf-8")  # Return cached language

    translator = Translator()
    detected_lang = translator.detect(text).lang
    await redis.set(key, detected_lang, ex=3600)  # Cache the language for 1 hour
    return detected_lang

# Translate text function (cached with Redis)
async def translate_text(text: str, src: str = None, redis=None):
    key = f"trans:{text}:{src}"
    cached_translation = await redis.get(key)
    
    if cached_translation:
        return cached_translation.decode("utf-8")  # Return cached translation
    
    translator = Translator()
    if src:
        result = translator.translate(text, src=src)
        translated_text = result.text
    else:
        result = translator.translate(text)
        translated_text = result.text
    
    if redis:
        await redis.set(key, translated_text, ex=3600)  # Cache for 1 hour
    
    return translated_text

# Main function to get the chat response with improved performance and caching
async def get_chat_response(graph, question: str, thread_id: str = "1", redis_url="redis://localhost:6379"):
    """
    Asynchronously generates a chat response based on the provided question using a state graph.
    Implements caching for language detection and translations using Redis.

    Args:
        graph (StateGraph): The graph to process the chat message and generate a response.
        question (str): The user's question to be answered.
        thread_id (str, optional): The identifier for the chat thread. Defaults to "1".

    Returns:
        str: The final response generated by the graph, or an error message if something goes wrong.
    """
    try:
        redis = await get_redis_connection(redis_url)
        
        # Check the language of the question, either from cache or by detecting
        language = await detect(question, redis)
        
        # If language is not English, translate the question to English
        if language != "en":
            question = await translate_text(question, src=language, redis=redis)

        response = ""
        config = {"configurable": {"thread_id": thread_id}}

        # Get the chat response from the graph
        with get_openai_callback() as cb:
            async for chunk in graph.astream(
                {
                    "messages": [("user", question)],
                },
                config=config,
                stream_mode="values",
            ):
                if chunk["messages"]:
                    response = chunk["messages"][-1].content
                    # If response is in a language other than English, translate it
                    if language != "en":
                        response = await translate_text(response, src=language, redis=redis)
        
        # Log OpenAI API usage
        print(f"Total Tokens: {cb.total_tokens}")
        print(f"Prompt Tokens: {cb.prompt_tokens}")
        print(f"Completion Tokens: {cb.completion_tokens}")
        print(f"Total Cost (USD): ${cb.total_cost}")

        # If no response, return fallback message
        if response:
            final_response = response
        else:
            final_response = "Please Try again later" if language == "en" else await translate_text("Please Try again later", src=language, redis=redis)
        
        await redis.close()  # Close Redis connection
        return final_response

    except Exception as e:
        print(f"Error occurred: {e}")
        try:
            # Fallback in case of errors
            fallback_prompt = ChatPromptTemplate.from_messages([
                ("system", "A user faced a system error and wants help. Trigger the contact tool."),
                ("human", "There was an error in processing my request. Can you contact support for me?")
            ])
            
            model_with_tool = llm.bind_tools([contact])
            agent_chain = fallback_prompt | model_with_tool

            response = await agent_chain.ainvoke({})

            # Translate response if needed
            contact_response = response.content if language == "en" else await translate_text(response.content, src=language, redis=redis)

            # Save to memory (simulate continuation of the conversation)
            await memory.aput(
                thread_id,
                {
                    "messages": [
                        ("user", "There was an error in processing my request. Can you contact support for me?"),
                        ("assistant", contact_response)
                    ]
                }
            )

            return contact_response
        except Exception as inner_e:
            print(f"Fallback also failed: {inner_e}")
            final_fallback = "Please try again later." if language == "en" else await translate_text("Please try again later", src=language, redis=redis)
            return final_fallback
